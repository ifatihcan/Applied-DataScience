# -*- coding: utf-8 -*-
"""Marketing Campaign Response Prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1TNVfHSdq6kes9JI-rur5sik9t8n00o_R

This assignment provides an opportunity to build an ANN model using the a bank marketing dataset

https://www.kaggle.com/sriharipramod/bank-loan-classification

Target variable - personal loan - reflects whether a customer accepted a personal loan offer.

Load the data and perform EDA.

1. Evaluate missing values.
2. Assess target class distribution.
3. Pre-process data as needed.
4. Assess information value of individual features (correlation analysis and pairlot).
"""

import numpy as np
import pandas as pd
from matplotlib import pyplot as plt
from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score
from sklearn.neural_network import MLPClassifier
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split
import seaborn as sns

data = pd.read_csv('UniversalBank.csv')

data.info()

data.head()

#Check for missing values in the dataset
missing_values = data.isnull().sum()
print(missing_values)

# Verifying if there are any missing values in the dataset
if missing_values.any():
    print("There are missing values in the dataset.")
else:
    print("No missing values found.")

sns.heatmap(data.isnull(), cbar=False)

#Check the distribution of the target variable (Personal Loan)
target_distribution = data['Personal Loan'].value_counts(normalize=True)
print(target_distribution)


plt.figure(figsize=(6,4))
sns.countplot(x='Personal Loan', data=data)
plt.title("Personal Loan Target Class Distribution")
plt.show()

#Pre-processing
# Dropping unnecessary columns 'ID' and 'ZIP Code'
data = data.drop(columns=['ID', 'ZIP Code'])

# Convert categorical columns to dummy variables
data = pd.get_dummies(data, columns=['Education'], drop_first=True)
print(data.head())

# Correlation analysis
correlation = data.corr()['Personal Loan'].sort_values(ascending=False)
print("Correlation with Personal Loan:\n", correlation)

# Correlation heatmap for all features

plt.figure(figsize=(10, 8))
correlation_matrix = data.corr()
sns.heatmap(correlation_matrix, annot=True, cmap='viridis', linewidths=0.5)
plt.title('Correlation Heatmap of Features')
plt.show()

sns.pairplot(data, hue = 'Personal Loan')

"""5. Split the data 70/30 into training and test datasets.
6. Develop an ANN model (MLPClassifier) with a single hidden layer with 20 nodes.
7. Assess model performance, provide the confusion matrix, classification report and ROC AUC values.

"""

# Split data into training and test sets (70/30 split)

X = data.drop('Personal Loan', axis=1)
y = data['Personal Loan']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)

# Scale data

scaler = MinMaxScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

X_train_df = pd.DataFrame(X_train, columns = X.columns)

X_train_df.describe().transpose()

# Create an ANN model with one hidden layer of 20 nodes

model = MLPClassifier(hidden_layer_sizes=20, activation='logistic', solver='adam', random_state=1)
model.fit(X_train, y_train)

#  Assess Model Performance

# Make predictions on the test set
y_pred = model.predict(X_test)

# Confusion Matrix
print("Confusion Matrix:")
print(confusion_matrix(y_test, y_pred))

# Classification Report
print("\nClassification Report:")
print(classification_report(y_test, y_pred))

# ROC AUC Score
roc_auc = roc_auc_score(y_test, model.predict_proba(X_test)[:, 1])
print("\nROC AUC Score:", roc_auc)

"""8. Assess the effect of the number of hidden nodes (5-30) on model ROC AUC.
9. Identify optimal number of nodes.


"""

from sklearn.metrics import accuracy_score

accuracies = []

for nodes in range(5, 30):
    model = MLPClassifier(hidden_layer_sizes=nodes, activation='logistic', solver='adam', random_state=1)
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    acc = accuracy_score(y_test, y_pred)
    accuracies.append(acc)

plt.plot(range(5, 30), accuracies)
plt.xlabel('Number of Nodes')
plt.ylabel('Accuracy on Test Set')
plt.title('Performance vs. Number of Nodes')
plt.show()

# Identify optimal number of nodes

optimal_nodes = range(5, 30)[accuracies.index(max(accuracies))]
print(f"Optimal number of nodes: {optimal_nodes}")